package provider

import (
	"time"

	"github.com/invopop/jsonschema"
)

type ChatOptions struct {
	// OpenAI: Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim.
	FrequencyPenalty *float64
	// OpenAI: Whether to return log probabilities of the output tokens or not. If true, returns the log probabilities of each output token returned in the `content` of `message`.
	Logprobs bool
	// OpenAI: An upper bound for the number of tokens that can be generated for a completion, including visible output tokens and [reasoning tokens](https://platform.openai.com/docs/guides/reasoning).
	MaxCompletionTokens uint
	// The maximum number of tokens to generate before stopping.
	MaxTokens uint
	// OpenAI: How many chat completion choices to generate for each input message. Note that you will be charged based on the number of generated tokens across all of the choices. Keep `n` as `1` to minimize costs.
	N *uint
	// OpenAI: Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics.
	PresencePenalty float64
	// OpenAI: This feature is in Beta. If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same `seed` and parameters should return the same result. Determinism is not guaranteed, and you should refer to the `system_fingerprint` response parameter to monitor changes in the backend.
	Seed *int64
	// OpenAI: What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. We generally recommend altering this or `top_p` but not both.
	// Anthropic: Amount of randomness injected into the response. Defaults to `1.0`. Ranges from `0.0` to `1.0`. Use `temperature` closer to `0.0` for analytical / multiple choice, and closer to `1.0` for creative and generative tasks. Note that even with `temperature` of `0.0`, the results will not be fully deterministic.
	Temperature *float64
	// OpenAI: An integer between 0 and 20 specifying the number of most likely tokens to return at each token position, each with an associated log probability. `logprobs` must be set to `true` if this parameter is used.
	TopLogprobs *int8
	TopP        *float64
	TopK        *int64
	// OpenAI: Whether to enable [parallel function calling](https://platform.openai.com/docs/guides/function-calling#configuring-parallel-function-calling) during tool use.
	ParallelToolCalls *bool
	// OpenAI: Cache responses for similar requests to optimize your cache hit rates. Replaces the `user` field.
	PromptCacheKey string
	// OpenAI: A stable identifier used to help detect users of your application that may be violating OpenAI's usage policies. The IDs should be a string that uniquely identifies each user. We recommend hashing their username or email address, in order to avoid sending us any identifying information. [Learn more](https://platform.openai.com/docs/guides/safety-best-practices#safety-identifiers).
	SafetyIdentifier string
	// OpenAI: This field is being replaced by `safety_identifier` and `prompt_cache_key`. Use `prompt_cache_key` instead to maintain caching optimizations. A stable identifier for your end-users. Used to boost cache hit rates by better bucketing similar requests and to help OpenAI detect and prevent abuse. [Learn more](https://platform.openai.com/docs/guides/safety-best-practices#safety-identifiers).
	User string
	// OpenAI: Modify the likelihood of specified tokens appearing in the completion. Accepts a JSON object that maps tokens (specified by their token ID in the tokenizer) to an associated bias value from -100 to 100. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token.
	LogitBias map[string]int64
	// OpenAI: **o-series models only** Constrains effort on reasoning for [reasoning models](https://platform.openai.com/docs/guides/reasoning). (Accepts [ReasoningEffortUnion.ofString])
	// Ollama: Think controls whether thinking/reasoning models will think before responding. (Accepts both [ReasoningEffortUnion.ofString] and [ReasoningEffortUnion.ofBool])
	ReasoningEffort *ReasoningEffortUnion
	// OpenAI: Specifies the processing type used for serving the request. Any of "auto", "default", "flex", "scale", "priority".
	// Anthropic: Any of "auto", "standard_only"
	ServiceTier string
	// OpenAI: Up to 4 sequences where the API will stop generating further tokens. The returned text will not contain the stop sequence.
	// Anthropic: Custom text sequences that will cause the model to stop generating.
	Stop []string
	// Schema specifying the format that the model must output.
	//
	// *Supported providers*: OpenAI, Ollama
	ResponseFormat *jsonschema.Schema
	// Anthropic: Enable extended thinking. Must be â‰¥1024 and less than `max_tokens`.
	Thinking uint64
	// Ollama: How long the model will stay loaded into memory following the request.
	KeepAlive *time.Duration
}

// Only one can be set
type ReasoningEffortUnion struct {
	ofString ReasoningEffortLevel
	ofBool   *bool
}

func NewReasoningEffortLevel(level ReasoningEffortLevel) *ReasoningEffortUnion {
	return &ReasoningEffortUnion{ofString: level}
}

func NewReasoningEffortBool(enabled bool) *ReasoningEffortUnion {
	return &ReasoningEffortUnion{ofBool: &enabled}
}

func (r *ReasoningEffortUnion) AsLevel() (ReasoningEffortLevel, bool) {
	if r.ofString == "" {
		return "", false
	}
	return r.ofString, true
}

func (r *ReasoningEffortUnion) AsBool() (bool, bool) {
	if r.ofBool == nil {
		return false, false
	}
	return *r.ofBool, true
}

func (r *ReasoningEffortUnion) AsAny() any {
	if level, ok := r.AsLevel(); ok {
		return level
	}
	if enabled, ok := r.AsBool(); ok {
		return enabled
	}
	return nil
}

type ReasoningEffortLevel string

const (
	ReasoningEffortLow    ReasoningEffortLevel = "low"
	ReasoningEffortMedium ReasoningEffortLevel = "medium"
	ReasoningEffortHigh   ReasoningEffortLevel = "high"
)
